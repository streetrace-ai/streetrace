"""Tests for WorkflowContext.call_llm() as an async generator.

Test that call_llm yields LlmCallEvent and LlmResponseEvent,
and stores the result for retrieval via get_last_result().
"""

from typing import TYPE_CHECKING
from unittest.mock import MagicMock, patch

import pytest

from streetrace.dsl.runtime.events import LlmCallEvent, LlmResponseEvent

if TYPE_CHECKING:
    from streetrace.dsl.runtime.context import WorkflowContext
    from streetrace.dsl.runtime.workflow import DslAgentWorkflow


@pytest.fixture
def mock_workflow() -> "DslAgentWorkflow":
    """Create a mock DslAgentWorkflow for testing."""
    return MagicMock()


@pytest.fixture
def workflow_context(mock_workflow: "DslAgentWorkflow") -> "WorkflowContext":
    """Create a WorkflowContext with test configuration."""
    from streetrace.dsl.runtime.context import WorkflowContext

    ctx = WorkflowContext(workflow=mock_workflow)

    # Set up models
    ctx.set_models({
        "main": "anthropic/claude-sonnet",
        "fast": "anthropic/claude-haiku",
    })

    # Set up prompts with lambdas (as generated by codegen)
    ctx.set_prompts({
        "greeting": lambda _: "Hello! How can I help you today?",
        "summarize": lambda c: f"Summarize the following: {c.vars.get('text', '')}",
        "format_output": lambda c: f"Format this: {c.message}",
    })

    # Set up prompt models (which model each prompt uses)
    ctx._prompt_models = {  # noqa: SLF001
        "greeting": "main",
        "summarize": "fast",
    }

    return ctx


async def collect_events(generator: object) -> list[object]:
    """Collect all events from an async generator."""
    return [event async for event in generator]  # type: ignore[union-attr]


class TestCallLlmYieldsEvents:
    """Test that call_llm yields correct events."""

    @pytest.mark.asyncio
    async def test_call_llm_yields_llm_call_event_first(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm yields LlmCallEvent as the first event."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="LLM response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("greeting"))

            assert len(events) >= 1
            first_event = events[0]
            assert isinstance(first_event, LlmCallEvent)

    @pytest.mark.asyncio
    async def test_call_llm_yields_llm_response_event_second(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm yields LlmResponseEvent as the second event."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="LLM response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("greeting"))

            assert len(events) == 2
            second_event = events[1]
            assert isinstance(second_event, LlmResponseEvent)

    @pytest.mark.asyncio
    async def test_llm_call_event_contains_correct_data(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """LlmCallEvent contains prompt name, model, and prompt text."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="LLM response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("greeting"))

            call_event = events[0]
            assert isinstance(call_event, LlmCallEvent)
            assert call_event.prompt_name == "greeting"
            assert call_event.model == "anthropic/claude-sonnet"
            assert call_event.prompt_text == "Hello! How can I help you today?"

    @pytest.mark.asyncio
    async def test_llm_response_event_contains_content(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """LlmResponseEvent contains the LLM response content."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="The response from LLM"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("greeting"))

            response_event = events[1]
            assert isinstance(response_event, LlmResponseEvent)
            assert response_event.prompt_name == "greeting"
            assert response_event.content == "The response from LLM"
            assert response_event.is_final is True

    @pytest.mark.asyncio
    async def test_llm_call_event_type_field(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """LlmCallEvent has correct type field value."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="LLM response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("greeting"))

            call_event = events[0]
            assert call_event.type == "llm_call"

    @pytest.mark.asyncio
    async def test_llm_response_event_type_field(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """LlmResponseEvent has correct type field value."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="LLM response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("greeting"))

            response_event = events[1]
            assert response_event.type == "llm_response"


class TestGetLastResult:
    """Test get_last_result() method for retrieving LLM response."""

    @pytest.mark.asyncio
    async def test_get_last_result_returns_llm_response(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """get_last_result returns the LLM response after iteration."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="The LLM result"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            # Consume the generator
            await collect_events(workflow_context.call_llm("greeting"))

            result = workflow_context.get_last_result()
            assert result == "The LLM result"

    @pytest.mark.asyncio
    async def test_get_last_result_returns_none_before_call(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """get_last_result returns None before any call_llm."""
        result = workflow_context.get_last_result()
        assert result is None

    @pytest.mark.asyncio
    async def test_get_last_result_updates_on_each_call(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """get_last_result returns the most recent LLM response."""
        with patch("litellm.acompletion") as mock_acompletion:
            # First call
            mock_response1 = MagicMock()
            mock_choice1 = MagicMock(message=MagicMock(content="First response"))
            mock_response1.choices = [mock_choice1]
            mock_acompletion.return_value = mock_response1

            await collect_events(workflow_context.call_llm("greeting"))

            assert workflow_context.get_last_result() == "First response"

            # Second call
            mock_response2 = MagicMock()
            mock_choice2 = MagicMock(message=MagicMock(content="Second response"))
            mock_response2.choices = [mock_choice2]
            mock_acompletion.return_value = mock_response2

            await collect_events(workflow_context.call_llm("greeting"))

            assert workflow_context.get_last_result() == "Second response"


class TestCallLlmErrorHandling:
    """Test error handling in call_llm generator."""

    @pytest.mark.asyncio
    async def test_call_llm_sets_none_result_on_error(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm sets _last_call_result to None on LLM error."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_acompletion.side_effect = Exception("LLM API error")

            # Consume the generator (should not raise)
            events = await collect_events(workflow_context.call_llm("greeting"))

            # Should have yielded the call event before the error
            assert len(events) == 1
            assert isinstance(events[0], LlmCallEvent)

            # Result should be None
            assert workflow_context.get_last_result() is None

    @pytest.mark.asyncio
    async def test_call_llm_yields_call_event_before_error(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm yields LlmCallEvent even when LLM call fails."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_acompletion.side_effect = Exception("LLM API error")

            events = await collect_events(workflow_context.call_llm("greeting"))

            # The call event should have been yielded
            assert len(events) >= 1
            assert isinstance(events[0], LlmCallEvent)
            assert events[0].prompt_name == "greeting"

    @pytest.mark.asyncio
    async def test_call_llm_unknown_prompt_yields_nothing(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm yields nothing for unknown prompt."""
        events = await collect_events(
            workflow_context.call_llm("nonexistent_prompt"),
        )

        assert len(events) == 0
        assert workflow_context.get_last_result() is None


class TestCallLlmModelResolution:
    """Test model resolution in call_llm generator."""

    @pytest.mark.asyncio
    async def test_call_llm_uses_prompt_model(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm uses the model specified for the prompt."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("summarize"))

            # The call event should have the correct model
            call_event = events[0]
            assert call_event.model == "anthropic/claude-haiku"

    @pytest.mark.asyncio
    async def test_call_llm_respects_model_override(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm uses explicit model override."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(
                workflow_context.call_llm(
                    "greeting",
                    model="openai/gpt-4",
                ),
            )

            call_event = events[0]
            assert call_event.model == "openai/gpt-4"

    @pytest.mark.asyncio
    async def test_call_llm_falls_back_to_main_model(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm falls back to main model for prompts without specific model."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            # format_output has no model specified
            events = await collect_events(
                workflow_context.call_llm("format_output"),
            )

            call_event = events[0]
            assert call_event.model == "anthropic/claude-sonnet"


class TestCallLlmPromptInterpolation:
    """Test prompt interpolation in call_llm generator."""

    @pytest.mark.asyncio
    async def test_call_llm_interpolates_context_vars(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm evaluates prompt lambda with context variables."""
        workflow_context.vars["text"] = "This is a long document."

        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Summary"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(workflow_context.call_llm("summarize"))

            call_event = events[0]
            assert "This is a long document." in call_event.prompt_text

    @pytest.mark.asyncio
    async def test_call_llm_uses_context_message(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm can access ctx.message in prompt lambda."""
        workflow_context.message = "Some user input"

        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Formatted"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            events = await collect_events(
                workflow_context.call_llm("format_output"),
            )

            call_event = events[0]
            assert "Some user input" in call_event.prompt_text
