"""Tests for WorkflowContext.call_llm() method.

Test that call_llm properly invokes LLM with named prompts
and handles model resolution.
"""

from typing import TYPE_CHECKING
from unittest.mock import MagicMock, patch

import pytest

if TYPE_CHECKING:
    from streetrace.dsl.runtime.context import WorkflowContext


class TestCallLlm:
    """Test WorkflowContext.call_llm() method."""

    @pytest.fixture
    def workflow_context(self) -> "WorkflowContext":
        """Create a WorkflowContext with test configuration."""
        from streetrace.dsl.runtime.context import WorkflowContext

        ctx = WorkflowContext()

        # Set up models
        ctx.set_models({
            "main": "anthropic/claude-sonnet",
            "fast": "anthropic/claude-haiku",
        })

        # Set up prompts with lambdas (as generated by codegen)
        ctx.set_prompts({
            "greeting": lambda _: "Hello! How can I help you today?",
            "summarize": lambda c: f"Summarize the following: {c.vars.get('text', '')}",
            "format_output": lambda c: f"Format this: {c.message}",
        })

        # Set up prompt models (which model each prompt uses)
        ctx._prompt_models = {  # noqa: SLF001
            "greeting": "main",
            "summarize": "fast",
        }

        return ctx

    @pytest.mark.asyncio
    async def test_call_llm_uses_named_prompt(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm looks up and evaluates the named prompt."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="LLM response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            result = await workflow_context.call_llm("greeting")

            # Verify acompletion was called with the evaluated prompt
            mock_acompletion.assert_called_once()
            call_kwargs = mock_acompletion.call_args.kwargs
            messages = call_kwargs.get("messages", [])
            assert len(messages) > 0
            assert messages[0]["content"] == "Hello! How can I help you today?"
            assert result == "LLM response"

    @pytest.mark.asyncio
    async def test_call_llm_interpolates_context_vars(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm evaluates prompt lambda with context variables."""
        # Set a context variable
        workflow_context.vars["text"] = "This is a long document."

        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_response.choices = [MagicMock(message=MagicMock(content="Summary"))]
            mock_acompletion.return_value = mock_response

            await workflow_context.call_llm("summarize")

            # Verify the prompt was evaluated with context
            mock_acompletion.assert_called_once()
            call_kwargs = mock_acompletion.call_args.kwargs
            messages = call_kwargs.get("messages", [])
            assert "This is a long document." in messages[0]["content"]

    @pytest.mark.asyncio
    async def test_call_llm_respects_prompt_model(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm uses the model specified for the prompt."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Fast response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            await workflow_context.call_llm("summarize")

            # Verify the correct model was used
            mock_acompletion.assert_called_once()
            call_kwargs = mock_acompletion.call_args.kwargs
            assert call_kwargs.get("model") == "anthropic/claude-haiku"

    @pytest.mark.asyncio
    async def test_call_llm_respects_model_override(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm allows explicit model override."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_choice = MagicMock(message=MagicMock(content="Override response"))
            mock_response.choices = [mock_choice]
            mock_acompletion.return_value = mock_response

            await workflow_context.call_llm("greeting", model="openai/gpt-4")

            # Verify the override model was used
            mock_acompletion.assert_called_once()
            call_kwargs = mock_acompletion.call_args.kwargs
            assert call_kwargs.get("model") == "openai/gpt-4"

    @pytest.mark.asyncio
    async def test_call_llm_returns_response(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm returns the LLM response content."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_response.choices = [
                MagicMock(message=MagicMock(content="The response from LLM")),
            ]
            mock_acompletion.return_value = mock_response

            result = await workflow_context.call_llm("greeting")

            assert result == "The response from LLM"

    @pytest.mark.asyncio
    async def test_call_llm_with_unknown_prompt_returns_none(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm returns None if prompt is not found."""
        result = await workflow_context.call_llm("nonexistent_prompt")
        assert result is None

    @pytest.mark.asyncio
    async def test_call_llm_falls_back_to_main_model(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm falls back to 'main' model if prompt has no model."""
        # format_output has no model specified
        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_response.choices = [MagicMock(message=MagicMock(content="Response"))]
            mock_acompletion.return_value = mock_response

            await workflow_context.call_llm("format_output")

            # Verify main model was used as fallback
            mock_acompletion.assert_called_once()
            call_kwargs = mock_acompletion.call_args.kwargs
            assert call_kwargs.get("model") == "anthropic/claude-sonnet"

    @pytest.mark.asyncio
    async def test_call_llm_uses_context_message(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm can access ctx.message in prompt lambda."""
        workflow_context.message = "Some user input"

        with patch("litellm.acompletion") as mock_acompletion:
            mock_response = MagicMock()
            mock_response.choices = [MagicMock(message=MagicMock(content="Formatted"))]
            mock_acompletion.return_value = mock_response

            await workflow_context.call_llm("format_output")

            # Verify the message was used
            mock_acompletion.assert_called_once()
            call_kwargs = mock_acompletion.call_args.kwargs
            messages = call_kwargs.get("messages", [])
            assert "Some user input" in messages[0]["content"]

    @pytest.mark.asyncio
    async def test_call_llm_handles_llm_error(
        self,
        workflow_context: "WorkflowContext",
    ) -> None:
        """call_llm handles LLM errors gracefully."""
        with patch("litellm.acompletion") as mock_acompletion:
            mock_acompletion.side_effect = Exception("LLM API error")

            result = await workflow_context.call_llm("greeting")

            # Should return None on error
            assert result is None
